{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-search-documents==11.6.0b6\n",
    "# !pip install azure-search-documents==11.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformer-description-advanced'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import datetime\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "container_name = config.get(\"storage_container\")\n",
    "storage_base_url = config.get(\"storage_base_url\")\n",
    "connection_string = config.get(\"storage_connection_string\")\n",
    "\n",
    "document_intelligence_endpoint = config.get(\"document_intelligence_endpoint\")\n",
    "document_intelligence_key = config.get(\"document_intelligence_key\")\n",
    "\n",
    "azure_openai_api_key = config.get(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = config.get(\"AZURE_OPENAI_API_BASE\")\n",
    "azure_openai_api_version = config.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_openai_embedding_model = config.get(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "chat_model = config.get(\"AZURE_OPENAI_MODEL\")\n",
    "\n",
    "search_credential = AzureKeyCredential(config.get(\"AZURE_AI_SEARCH_KEY\"))\n",
    "search_endpoint = config.get(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "\n",
    "index_type = \"advanced\" # \"simple\" or advanced\"\n",
    "index_name = f\"transformer-description-{index_type}\"\n",
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=azure_openai_embedding_model,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    openai_api_version=azure_openai_api_version\n",
    ")\n",
    "\n",
    "embedding_function = embeddings.embed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    ComplexField,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    ScoringProfile,\n",
    "    FreshnessScoringFunction,\n",
    "    FreshnessScoringParameters,\n",
    "    TextWeights,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=search_credential\n",
    ")\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True, sortable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"last_update\", type=SearchFieldDataType.DateTimeOffset, filterable=True),\n",
    "    SearchField(\n",
    "        name=\"text_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_function(\"Text\")), # 3072\n",
    "        vector_search_profile_name=\"myHnswProfile\"\n",
    "    ),\n",
    "    ComplexField(\n",
    "        name=\"metadata\",\n",
    "        fields=[\n",
    "            SearchableField(name=\"section_heading\", type=SearchFieldDataType.String),\n",
    "            SearchField(\n",
    "                name=\"images\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.String)\n",
    "            ),\n",
    "            SimpleField(name=\"type\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"chunk_size\", type=SearchFieldDataType.Int32),\n",
    "            SimpleField(name=\"page_start\", type=SearchFieldDataType.Int32),\n",
    "            SimpleField(name=\"page_end\", type=SearchFieldDataType.Int32),\n",
    "            SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "            SearchField(\n",
    "                name=\"keywords\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "                searchable=True,\n",
    "                filterable=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Adding a custom scoring profile with a freshness function\n",
    "sc_name = \"scoring_profile_1\"\n",
    "scoring_profile = ScoringProfile(\n",
    "    name=sc_name,\n",
    "    text_weights=TextWeights(\n",
    "        weights={\n",
    "            \"title\": 5\n",
    "        }\n",
    "    ),\n",
    "    function_aggregation=\"sum\",\n",
    "    functions=[\n",
    "        FreshnessScoringFunction(\n",
    "            field_name=\"last_update\",\n",
    "            boost=100,\n",
    "            parameters=FreshnessScoringParameters(boosting_duration=\"P2D\"),\n",
    "            interpolation=\"linear\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Adding vector search settings\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"myVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_model,\n",
    "                model_name=azure_openai_embedding_model,\n",
    "                api_key=azure_openai_api_key\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[\n",
    "            SemanticField(field_name=\"metadata/keywords\")\n",
    "        ],\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transformer-description-advanced created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    scoring_profiles=[scoring_profile],\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Pipeline**\n",
    "- **Read data from storage account**\n",
    "- **Use Document Intelligence to crack PDF**\n",
    "    - **Extract text**\n",
    "    - **Extract images and write to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_blob_service_client(connection_string, container_name):\n",
    "    # Initialize the BlobServiceClient using the connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    return container_client\n",
    "\n",
    "\n",
    "def initialize_document_intelligence_client(endpoint, key):\n",
    "    # Initialize the Document Intelligence client\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key)\n",
    "    )\n",
    "    return document_intelligence_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob_content(blob_client, pbar):\n",
    "    try:\n",
    "        download_stream = blob_client.download_blob()\n",
    "        blob_content = download_stream.readall()\n",
    "        analysis_status = \"Downloaded, now analyzing\"\n",
    "        return blob_content, analysis_status\n",
    "    except Exception as e:\n",
    "        analysis_status = f\"Download Failed: {e}\"\n",
    "        pbar.set_postfix({\"Status\": analysis_status})\n",
    "        pbar.update(1)  # Update progress bar even if download fails\n",
    "        return {\"error_message\": str(e)}, analysis_status\n",
    "\n",
    "def analyze_document(document_intelligence_client, blob_content, blob_name):\n",
    "    from azure.ai.documentintelligence.models import AnalyzeOutputOption, AnalyzeResult\n",
    "\n",
    "    figures_list = []\n",
    "    try:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            model_id=\"prebuilt-layout\",\n",
    "            analyze_request=blob_content,\n",
    "            content_type=\"application/octet-stream\",  # Adjust based on your document type\n",
    "            output=[AnalyzeOutputOption.FIGURES]\n",
    "        )\n",
    "        result: AnalyzeResult = poller.result()\n",
    "        operation_id = poller.details[\"operation_id\"]\n",
    "\n",
    "        if result.figures:\n",
    "            for figure in result.figures:\n",
    "                if figure.id:\n",
    "                    response = document_intelligence_client.get_analyze_result_figure(\n",
    "                        model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "                    )\n",
    "                    with open(f\"data/figures/1706.03762v7/{figure.id}.png\", \"wb\") as writer:\n",
    "                        writer.writelines(response)\n",
    "        return result, blob_name\n",
    "    except Exception as e:\n",
    "        return {\"error_message\": str(e)}, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1706.03762v7.pdf: 100%|██████████| 1/1 [00:10<00:00, 10.29s/blob, Status=Finished]                 \n"
     ]
    }
   ],
   "source": [
    "def run_process_data_pipeline():\n",
    "    documents = []\n",
    "    # documents = get_documents_dict()\n",
    "    failed_files = []\n",
    "    try:\n",
    "        # Initialize the BlobServiceClient & Document Intelligence client\n",
    "        container_client = initialize_blob_service_client(connection_string, container_name)\n",
    "        document_intelligence_client = initialize_document_intelligence_client(document_intelligence_endpoint, document_intelligence_key)\n",
    "\n",
    "        # List all blobs in the container\n",
    "        blob_list = list(container_client.list_blobs())  # Convert generator to list to get total count\n",
    "        total_blobs = len(blob_list)  # Total number of blobs\n",
    "\n",
    "        if total_blobs == 0:\n",
    "            print(\"No blobs found in the container.\")\n",
    "        else:\n",
    "            # Initialize the tqdm progress bar\n",
    "            with tqdm(total=total_blobs, desc=\"Processing Blobs\", unit=\"blob\") as pbar:\n",
    "                # Initialize a variable to hold analysis status\n",
    "                analysis_status = \"\"\n",
    "\n",
    "                for blob in blob_list:\n",
    "                    blob_name = blob.name\n",
    "\n",
    "                    # Update the progress bar's description to show the current blob\n",
    "                    pbar.set_description(f\"Processing {blob_name}\")\n",
    "\n",
    "                    # Download the blob's content\n",
    "                    blob_content, analysis_status = download_blob_content(blob_client = container_client.get_blob_client(blob_name), pbar=pbar)\n",
    "                    pbar.set_postfix({\"Status\": analysis_status})\n",
    "\n",
    "                    # if blob_content is None:\n",
    "                    if isinstance(blob_content, dict) and \"error_message\" in blob_content:\n",
    "                        print(f\"Skipping {blob_name} due to download failure.\")\n",
    "                        failed_files.append({\n",
    "                            blob_name: {\n",
    "                                \"stage\": \"Blob Download\",\n",
    "                                \"error_message\": blob_content[\"error_message\"]\n",
    "                            }\n",
    "                        })\n",
    "                        continue  # Skip to the next blob\n",
    "\n",
    "                    # Analyze the document using Document Intelligence\n",
    "                    data, filename = analyze_document(document_intelligence_client, blob_content, blob_name)\n",
    "\n",
    "                    if \"error_message\" not in data:\n",
    "                        documents.append({\"filename\": filename, \"data\": data, \"url\": f\"{storage_base_url}/{container_name}/{blob_name}\"})\n",
    "                    else:\n",
    "                        failed_files.append({\n",
    "                            blob_name: {\n",
    "                                \"stage\": \"Document Analysis\",\n",
    "                                \"error_message\": data[\"error_message\"]\n",
    "                            }\n",
    "                        })\n",
    "                    # Update the progress bar after processing each blob\n",
    "                    pbar.update(1)\n",
    "                pbar.set_postfix({\"Status\": \"Finished\"})\n",
    "        return documents, failed_files\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# before running the pipeline, make sure that the documents are not protected.\n",
    "documents, failed_documents = run_process_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of failed documents: 0\n",
      "\n",
      "Total number of documents: 1\n",
      "    - Failed document: 1706.03762v7.pdf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of failed documents: {len(failed_documents)}\")\n",
    "for failed_document in failed_documents:\n",
    "    print(f\"failed_document: {failed_document}\")\n",
    "\n",
    "print()\n",
    "print(f\"Total number of documents: {len(documents)}\")\n",
    "for doc in documents:\n",
    "    print(f\"    - Failed document: {doc.get('filename')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Image Descriptions**\n",
    "- **Base64 encode images**\n",
    "- **Send encoded images to gpt-4o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from mimetypes import guess_type\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,  \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "\n",
    "def get_image_descriptions(image_url):\n",
    "    # function to create image descriptions\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides text descriptions of images.\"},\n",
    "            {\"role\": \"user\", \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Describe this image:\" \n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"{image_url}\"\n",
    "                    }\n",
    "                }\n",
    "            ]} \n",
    "        ],\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "images_path = \"data/figures/1706.03762v7\"\n",
    "image_files = glob.glob(os.path.join(images_path, \"*.png\"))\n",
    "base64_images = []\n",
    "\n",
    "for image_path in image_files:\n",
    "    base64_image = local_image_to_data_url(image_path)\n",
    "    base64_images.append({\n",
    "        \"path\": f\"{image_path}\",\n",
    "        \"description\": get_image_descriptions(base64_image)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Chunks + Add Metadata**\n",
    "- **chunk id**\n",
    "- **last update**\n",
    "- **number of tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def get_sweden_time():\n",
    "    # Define the timezone for Sweden\n",
    "    sweden_tz = pytz.timezone('Europe/Stockholm')\n",
    "    utc_now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    sweden_time = utc_now.astimezone(sweden_tz)\n",
    "    return sweden_time.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_token(chunk, model=\"gpt-4o\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(chunk)\n",
    "    return str(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure output from Document Intelligence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_content(doc_info):\n",
    "    # Initialize variables\n",
    "    new_docs = []\n",
    "    current_title = None\n",
    "    current_section_start_page = None\n",
    "\n",
    "    doc = doc_info[\"data\"]\n",
    "\n",
    "    # Loop through paragraphs to structure the main content\n",
    "    # for paragraph in documents[\"pdf\"][0][\"content\"][\"ec330b.pdf\"].paragraphs:\n",
    "    for paragraph in doc.paragraphs:\n",
    "        # Extract page number from boundingRegions\n",
    "        if \"boundingRegions\" in paragraph and paragraph[\"boundingRegions\"]:\n",
    "            paragraph_page_number = paragraph[\"boundingRegions\"][0][\"pageNumber\"]\n",
    "        else:\n",
    "            paragraph_page_number = None\n",
    "\n",
    "        if paragraph.role == \"title\":\n",
    "            # Update the current title but do not add an entry to new_docs\n",
    "            current_title = paragraph.content\n",
    "        elif paragraph.role == \"sectionHeading\":\n",
    "            # Start a new entry for the section heading with the current title\n",
    "            current_section_start_page = paragraph_page_number\n",
    "            new_docs.append({\n",
    "                \"title\": current_title,\n",
    "                \"sectionHeading\": paragraph.content,\n",
    "                \"content\": \"\",\n",
    "                \"page_start\": current_section_start_page,\n",
    "                \"page_end\": paragraph_page_number,\n",
    "                \"images\": []\n",
    "            })\n",
    "        else:\n",
    "            # Add content to the last entry, updating page_end as needed\n",
    "            if new_docs:\n",
    "                new_docs[-1][\"content\"] += \" \" + paragraph.content\n",
    "                new_docs[-1][\"page_end\"] = paragraph_page_number\n",
    "\n",
    "    # Helper function to find the nearest section heading\n",
    "    def find_nearest_section(sections, page_number):\n",
    "        nearest_section = None\n",
    "        for section in sections:\n",
    "            if section[\"page_start\"] <= page_number <= section[\"page_end\"]:\n",
    "                nearest_section = section\n",
    "        return nearest_section\n",
    "\n",
    "    # Add images to the appropriate sections\n",
    "    for figure in doc.figures:\n",
    "        if \"boundingRegions\" in figure and figure[\"boundingRegions\"]:\n",
    "            figure_page = figure[\"boundingRegions\"][0][\"pageNumber\"]\n",
    "            # Find the nearest section\n",
    "            nearest_section = find_nearest_section(new_docs, figure_page)\n",
    "            if nearest_section:\n",
    "                filename = doc_info.get('filename')\n",
    "                basename, _ = os.path.splitext(filename)\n",
    "                nearest_section[\"images\"].append(\n",
    "                    f\"{basename}/{figure['id']}\"\n",
    "                )\n",
    "\n",
    "    return new_docs\n",
    "    \n",
    "\n",
    "# new_docs = structure_content(documents[0])\n",
    "# new_docs[7]\n",
    "\n",
    "# for doc in new_docs[:2]:\n",
    "#     print(f\"Title: {doc['title']}\\nSection Heading: {doc['sectionHeading']}\\nContent: {doc['content']}\\nPage Start: {doc['page_start']}\\nPage End: {doc['page_end']}\\nImages: {doc['images']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings as AzureOpenAIEmbeddingsLC\n",
    "import json\n",
    "\n",
    "def create_splits(content, chunk_size=2048, chunk_overlap=256):\n",
    "    try:\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "        # This list will hold our final dictionaries with updated content.\n",
    "        splits = []\n",
    "\n",
    "        # Iterate through original documents and create chunks\n",
    "        for doc in content:\n",
    "            text = doc.get(\"content\", \"\")\n",
    "            \n",
    "            # Update the dictionary to use `page_content` instead of `content`\n",
    "            doc[\"page_content\"] = text\n",
    "            doc.pop(\"content\", None)  # Remove the old `content` key if it exists\n",
    "\n",
    "            # print(doc.keys())\n",
    "\n",
    "            chunks = text_splitter.create_documents([text])\n",
    "            if len(chunks) < 1:\n",
    "                splits.append(doc)\n",
    "                # splits.append(json.dumps(doc))\n",
    "            else:\n",
    "                for chunk in chunks:\n",
    "                    new_doc = doc.copy()\n",
    "                    new_doc[\"page_content\"] = chunk.page_content  # Assign the splitted text\n",
    "                    splits.append(new_doc)\n",
    "                    # splits.append(json.dumps(new_doc))\n",
    "        return splits\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '62112fb9-3e61-52f8-a291-3425273c20ec',\n",
       " 'page_content': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. + Work performed while at Google Brain. #Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       " 'last_update': '2025-01-21T15:59:50.967403+01:00',\n",
       " 'metadata': {'type': 'document_chunk',\n",
       "  'title': '1706.03762v7.pdf',\n",
       "  'sectionHeading': 'Abstract',\n",
       "  'images': [],\n",
       "  'chunk_size': '422',\n",
       "  'page_start': 1,\n",
       "  'page_end': 1,\n",
       "  'url': 'https://storagepovel.blob.core.windows.net/transformerpaper/1706.03762v7.pdf'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to aggregate content for each document type\n",
    "def format_documents(documents, base64_images):\n",
    "    aggregated_documents = []\n",
    "\n",
    "    for doc in documents:\n",
    "        url = doc[\"url\"]  # URL for the document\n",
    "        filename = doc[\"filename\"]  # Filename of the document\n",
    "\n",
    "        content = structure_content(doc)\n",
    "        splits = create_splits(content, chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "        for i, split in enumerate(splits):\n",
    "            name = f\"{filename}_chunk_{i}\"\n",
    "            namespace = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")\n",
    "\n",
    "            # Append the splits with metadata\n",
    "            aggregated_documents.append({\n",
    "                \"id\": str(uuid.uuid5(namespace, name)),\n",
    "                \"page_content\": split.get(\"page_content\", \"\") if isinstance(split, dict) else split.page_content,  # Aggregated chunk content\n",
    "                \"last_update\": get_sweden_time(),  # Current date & time\n",
    "                \"metadata\": {\n",
    "                    \"type\": \"document_chunk\",\n",
    "                    \"title\": filename,\n",
    "                    \"sectionHeading\": split.get(\"sectionHeading\", \"\"),  # Section heading in the chunk\n",
    "                    \"images\": split.get(\"images\", []),  # Images in the chunk\n",
    "                    \"chunk_size\": count_token(split.get(\"page_content\", \"\") if isinstance(split, dict) else split.page_content),\n",
    "                    \"page_start\": split.get(\"page_start\", 0),\n",
    "                    \"page_end\": split.get(\"page_end\", 0),\n",
    "                    \"url\": url\n",
    "                }\n",
    "            })\n",
    "\n",
    "\n",
    "            # Append the image descriptions with metadata\n",
    "            for image in base64_images:\n",
    "                figure_id = os.path.splitext(image[\"path\"])[0].split(\"\\\\\")[-1]\n",
    "                folder_name = Path(image[\"path\"]).parent.name\n",
    "                image_path = f\"{folder_name}/{figure_id}\"\n",
    "                \n",
    "                # Check if the image path is in the current split\n",
    "                if image_path in split.get(\"images\"):\n",
    "                    aggregated_documents.append({\n",
    "                        \"id\": str(uuid.uuid5(namespace, name)),\n",
    "                        \"page_content\": image[\"description\"],\n",
    "                        \"last_update\": get_sweden_time(),\n",
    "                        \"metadata\": {\n",
    "                            \"type\": \"image_description\",\n",
    "                            \"title\": filename,\n",
    "                            \"sectionHeading\": split.get(\"sectionHeading\", \"\"),\n",
    "                            \"images\": [image_path],\n",
    "                            \"chunk_size\": count_token(image[\"description\"]),\n",
    "                            \"page_start\": figure_id.split(\".\")[0],\n",
    "                            \"page_end\": figure_id.split(\".\")[0],\n",
    "                            \"url\": url\n",
    "                        }\n",
    "                    })\n",
    "\n",
    "    return aggregated_documents\n",
    "\n",
    "# Call the function with your documents dictionary\n",
    "formatted_documents = format_documents(documents, base64_images)\n",
    "formatted_documents[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract additional metadata using AOAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Properties(BaseModel):\n",
    "    keywords: Optional[List[str]] = Field(\n",
    "        ...,\n",
    "        description=\"Important keywords and entities from the chunk for better searchability. If no keywords are found, return an empty list [].\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 32/32 [00:27<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed in 27.81 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_transformers.openai_functions import create_metadata_tagger\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "if index_type == \"advanced\":\n",
    "    model = \"gpt-4o-mini-global\"\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=model,\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        api_key=azure_openai_api_key,\n",
    "        azure_endpoint=azure_openai_endpoint,\n",
    "        api_version=azure_openai_api_version\n",
    "    )\n",
    "\n",
    "    system = \"\"\"You are an expert at extracting metadata from document chunks.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    document_transformer = create_metadata_tagger(metadata_schema=Properties, llm=llm, prompt=prompt)\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Process documents with progress bar\n",
    "    split_list_tmp = []\n",
    "    formatted_documents_lc = [\n",
    "        Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"])\n",
    "        for doc in formatted_documents\n",
    "    ]\n",
    "\n",
    "    for doc in tqdm(formatted_documents_lc, desc=\"Processing documents\"):\n",
    "        enhanced_doc = document_transformer.transform_documents([doc])\n",
    "        split_list_tmp.extend(enhanced_doc)\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    split_list = split_list_tmp\n",
    "    print(f\"Processing completed in {elapsed_time:.2f} seconds.\")\n",
    "else:\n",
    "    split_list=formatted_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=azure_openai_embedding_model,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version\n",
    ")\n",
    "\n",
    "embedding_function = embeddings.embed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting documents: 100%|██████████| 32/32 [00:02<00:00, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed in 2.61 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "def convert_document(doc, embedding_function):\n",
    "    # Define standard values\n",
    "    standard_values = {\n",
    "        'id': f'{uuid.uuid4()}',\n",
    "        'title': 'unknown',\n",
    "        'page_content': '',\n",
    "        'last_update': get_sweden_time(),\n",
    "        'text_vector': [],\n",
    "        'metadata': {\n",
    "            \"section_heading\": '',\n",
    "            \"images\": [],\n",
    "            \"type\": \"\",\n",
    "            \"chunk_size\": 0,\n",
    "            \"page_start\": 0,\n",
    "            \"page_end\": 0,\n",
    "            \"url\": '',\n",
    "            \"keywords\": []\n",
    "        }\n",
    "    }\n",
    "    # Extract values from the Document object, using standard values if missing\n",
    "    id = doc.metadata.get('id', standard_values['id'])  # Use standard\n",
    "    title = doc.metadata.get('title', standard_values['title'])\n",
    "    content = doc.page_content if doc.page_content is not None else standard_values['page_content']\n",
    "    last_update = doc.metadata.get('last_update', standard_values['last_update'])\n",
    "    text_vector = embedding_function(content) if content else standard_values['text_vector']\n",
    "    metadata_defaults = standard_values['metadata']\n",
    "    metadata = {\n",
    "        \"section_heading\": doc.metadata.get('sectionHeading', metadata_defaults['section_heading']),\n",
    "        \"images\": doc.metadata.get('images', metadata_defaults['images']),\n",
    "        \"type\": doc.metadata.get('type', metadata_defaults['type']),\n",
    "        \"chunk_size\": doc.metadata.get('chunk_size', metadata_defaults['chunk_size']),\n",
    "        \"page_start\": doc.metadata.get('page_start', metadata_defaults['page_start']),\n",
    "        \"page_end\": doc.metadata.get('page_end', metadata_defaults['page_end']),\n",
    "        \"url\": doc.metadata.get('url', metadata_defaults['url']),\n",
    "        \"keywords\": doc.metadata.get('keywords', metadata_defaults['keywords'])\n",
    "    }\n",
    "\n",
    "    # Construct the document for uploading\n",
    "    document = {\n",
    "        \"id\": id,\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"last_update\": last_update,\n",
    "        \"text_vector\": text_vector,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    return document\n",
    "\n",
    "data_final = []\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Process documents with progress bar\n",
    "for i, doc in enumerate(tqdm(split_list, desc=\"Converting documents\")):\n",
    "    data_final.append(convert_document(doc, embedding_function))\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Conversion completed in {elapsed_time:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "def push_to_index(data, search_endpoint, search_credential, index_name=index_name):\n",
    "    search_client = SearchClient(endpoint=search_endpoint, index_name=index_name, credential=search_credential)\n",
    "    search_client.upload_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 32/32 [00:00<00:00, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 took 0.86 seconds.\n",
      "\n",
      "All batches pushed in 0.86 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Group the header based chunks into batches\n",
    "batch_size = 50\n",
    "total_chunks = len(data_final)\n",
    "num_batches = (total_chunks + batch_size - 1) // batch_size\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(total=total_chunks, desc=\"Processing Batches\")\n",
    "\n",
    "# Process each batch\n",
    "for i, batch_num in enumerate(range(num_batches)):\n",
    "    batch_start_time = time.time()\n",
    "    start = batch_num * batch_size\n",
    "    batch = data_final[start:start + batch_size]\n",
    "\n",
    "    # Push the documents to the index\n",
    "    push_to_index(\n",
    "        data=batch,\n",
    "        search_endpoint=search_endpoint,\n",
    "        search_credential=search_credential,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    progress_bar.update(len(batch))\n",
    "\n",
    "    batch_end_time = time.time()\n",
    "    elapsed_batch_time = batch_end_time - batch_start_time\n",
    "    print(f\"Batch {batch_num + 1} took {elapsed_batch_time:.2f} seconds.\\n\")\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "overall_end_time = time.time()\n",
    "elapsed_overall_time = overall_end_time - overall_start_time\n",
    "print(f\"All batches pushed in {elapsed_overall_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
