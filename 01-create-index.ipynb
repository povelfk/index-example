{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating a Vector Search Index with Azure AI Foundry**\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to create and populate a vector search index in Azure AI Search using Azure AI Foundry. You'll learn how to process document data, extract content, generate vector embeddings, and build a search index that can power semantic search and retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "## What is Vector Search?\n",
    "Vector search is a technique that allows you to find similar items based on their semantic meaning rather than just keyword matches. In this approach:\n",
    "\n",
    "1. **Embeddings**: Documents or chunks of text are converted into numerical vector representations (embeddings) that capture semantic meaning\n",
    "2. **Vector Storage**: These vectors are stored in specialized indexes optimized for vector similarity operations\n",
    "3. **Similarity Search**: At query time, the system finds documents whose vectors are closest to the query vector\n",
    "\n",
    "This enables more powerful search capabilities than traditional keyword-based search, as it can understand concepts, synonyms, and the contextual meaning behind words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up The Environment\n",
    "\n",
    "First, we'll load environment variables and required dependencies for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "config = dotenv.dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Key Environment Variables\n",
    "Here, we load the environment variables including storage configuration, model names, and the date for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "azure_openai_api_key = config.get(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = config.get(\"AZURE_OPENAI_API_BASE\")\n",
    "azure_openai_api_version = config.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_openai_chat_model = config.get(\"AZURE_OPENAI_MODEL\")\n",
    "azure_openai_embedding_model = config.get(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "search_credential = AzureKeyCredential(config.get(\"SEARCH_KEY\"))\n",
    "search_endpoint = config.get(\"SEARCH_ENDPOINT\")\n",
    "\n",
    "document_intelligence_key=config.get(\"document_intelligence_key\")\n",
    "document_intelligence_endpoint=config.get(\"document_intelligence_endpoint\")\n",
    "\n",
    "container_name = config.get(\"storage_container\")\n",
    "storage_base_url = config.get(\"storage_base_url\")\n",
    "connection_string = config.get(\"storage_connection_string\")\n",
    "\n",
    "index_name = config.get(\"SEARCH_INDEX_NAME\")\n",
    "index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Helper Functions\n",
    "\n",
    "### Creating an Embedding Function\n",
    "This function helps us generate vector embeddings for text using Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = azure_openai_api_key,  \n",
    "  api_version = azure_openai_api_version,\n",
    "  azure_endpoint = azure_openai_endpoint\n",
    ")\n",
    "\n",
    "def get_embedding(text, model=azure_openai_embedding_model): # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Azure AI Search Index Schema\n",
    "Here we define the structure of our search index, including fields for content, metadata, and vector representations. We also configure vector search capabilities and semantic search features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=search_credential,\n",
    ")\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True, sortable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SimpleField(name=\"last_update\", type=SearchFieldDataType.DateTimeOffset, filterable=True),\n",
    "    SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "    SearchField(\n",
    "        name=\"text_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=3072,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# Adding vector search settings\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            vectorizer_name=\"myVectorizer\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            vectorizer_name=\"myVectorizer\",\n",
    "            parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_embedding_model,\n",
    "                model_name=azure_openai_embedding_model,\n",
    "                api_key=azure_openai_api_key,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the Azure AI Search Index\n",
    "\n",
    "Now that we've defined the schema, we'll create the actual search index in Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " demo created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search=semantic_search\n",
    ")\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Processing Pipeline\n",
    "\n",
    "### Setting up Document Extraction Services\n",
    "Next, we'll create functions to read data from our storage account and use Azure AI Document Intelligence to extract content from PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "def initialize_blob_service_client(connection_string, container_name):\n",
    "    # Initialize the BlobServiceClient and returns the container client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    return container_client\n",
    "\n",
    "def initialize_document_intelligence_client():\n",
    "    # Initialize the Document Intelligence client\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=document_intelligence_endpoint,\n",
    "        credential=AzureKeyCredential(document_intelligence_key)\n",
    "    )\n",
    "    return document_intelligence_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Processing Functions\n",
    "These functions handle downloading documents from blob storage and analyzing them with AI Document Intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob_content(blob_client):\n",
    "    # Download the blob's content\n",
    "    download_stream = blob_client.download_blob()\n",
    "    blob_content = download_stream.readall()\n",
    "    return blob_content\n",
    "\n",
    "\n",
    "def analyze_document(document_intelligence_client, blob_content):\n",
    "    # Analyze the document using the Document Intelligence client\n",
    "    from azure.ai.documentintelligence.models import AnalyzeResult, AnalyzeOutputOption\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-layout\",\n",
    "        analyze_request=blob_content,\n",
    "        content_type=\"application/octet-stream\",  # Adjust based on your document type\n",
    "        output=[AnalyzeOutputOption.FIGURES]\n",
    "    )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    operation_id = poller.details[\"operation_id\"]\n",
    "\n",
    "    if result.figures:\n",
    "        for figure in result.figures:\n",
    "            if figure.id:\n",
    "                response = document_intelligence_client.get_analyze_result_figure(\n",
    "                    model_id=result.model_id, result_id=operation_id, figure_id=figure.id\n",
    "                )\n",
    "                with open(f\"data/figures/{figure.id}.png\", \"wb\") as writer:\n",
    "                    writer.writelines(response)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Document Processing Pipeline\n",
    "This function orchestrates the document processing workflow, retrieving PDF files from storage and extracting their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1706.03762v7.pdf:   0%|          | 0/1 [00:08<?, ?blob/s]\n"
     ]
    },
    {
     "ename": "ResourceNotFoundError",
     "evalue": "(NotFound) Resource not found.\nCode: NotFound\nMessage: Resource not found.\nInner error: {\n    \"code\": \"OutputOptionNotFound\",\n    \"message\": \"The output option was not found: Failed to generate the analyze results in the output option format. Output option should be informed in analyze call.\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceNotFoundError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m             pbar.set_postfix({\u001b[33m\"\u001b[39m\u001b[33mStatus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mFinished\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m documents_raw = \u001b[43mrun_process_data_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_process_data_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip to the next blob if download failed\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Analyze the document using Document Intelligence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m data = \u001b[43manalyze_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_intelligence_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblob_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip to the next blob if analysis failed\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36manalyze_document\u001b[39m\u001b[34m(document_intelligence_client, blob_content)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m figure \u001b[38;5;129;01min\u001b[39;00m result.figures:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m figure.id:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         response = \u001b[43mdocument_intelligence_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_analyze_result_figure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata/figures/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfigure.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[32m     26\u001b[39m             writer.writelines(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\povelf\\AppData\\Local\\miniconda3\\envs\\pisa\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:119\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\povelf\\AppData\\Local\\miniconda3\\envs\\pisa\\Lib\\site-packages\\azure\\ai\\documentintelligence\\_operations\\_operations.py:1470\u001b[39m, in \u001b[36mDocumentIntelligenceClientOperationsMixin.get_analyze_result_figure\u001b[39m\u001b[34m(self, model_id, result_id, figure_id, **kwargs)\u001b[39m\n\u001b[32m   1468\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (StreamConsumedError, StreamClosedError):\n\u001b[32m   1469\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1471\u001b[39m error = _deserialize(_models.ErrorResponse, response.json())\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response, model=error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\povelf\\AppData\\Local\\miniconda3\\envs\\pisa\\Lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[39m, in \u001b[36mmap_error\u001b[39m\u001b[34m(status_code, response, error_map)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    162\u001b[39m error = error_type(response=response)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mResourceNotFoundError\u001b[39m: (NotFound) Resource not found.\nCode: NotFound\nMessage: Resource not found.\nInner error: {\n    \"code\": \"OutputOptionNotFound\",\n    \"message\": \"The output option was not found: Failed to generate the analyze results in the output option format. Output option should be informed in analyze call.\"\n}"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_process_data_pipeline():\n",
    "    documents = []\n",
    "\n",
    "    # Initialize the BlobServiceClient & Document Intelligence client\n",
    "    container_client = initialize_blob_service_client(connection_string, container_name)\n",
    "    document_intelligence_client = initialize_document_intelligence_client()\n",
    "    # List all blobs in the container and directory papers (Attention is all you need, Large Concep Models, Large Language Diffusion Models)\n",
    "    blob_list = list(container_client.list_blobs())\n",
    "    # Filter to only include PDF files\n",
    "    pdf_blob_list = [blob for blob in blob_list if blob.name.lower().endswith('.pdf')]\n",
    "\n",
    "    if len(pdf_blob_list) == 0:\n",
    "        print(\"No blobs found in the container/directory.\")\n",
    "    else:\n",
    "        with tqdm(total=len(pdf_blob_list), desc=\"Processing Blobs\", unit=\"blob\") as pbar:\n",
    "            for blob in pdf_blob_list:\n",
    "                blob_name = blob.name\n",
    "\n",
    "                # Update the progress bar's description to show the current blob\n",
    "                pbar.set_description(f\"Processing {blob_name}\")\n",
    "\n",
    "                # Download the blob's content\n",
    "                blob_content = download_blob_content(blob_client = container_client.get_blob_client(blob_name))\n",
    "\n",
    "                if blob_content is None:\n",
    "                    continue # Skip to the next blob if download failed\n",
    "\n",
    "                # Analyze the document using Document Intelligence\n",
    "                data = analyze_document(document_intelligence_client, blob_content)\n",
    "\n",
    "                if data is None:\n",
    "                    continue # Skip to the next blob if analysis failed\n",
    "\n",
    "                documents.append({\n",
    "                    \"filename\": blob_name,\n",
    "                    \"data\": data,\n",
    "                    \"url\": f\"{storage_base_url}/{container_name}/{blob_name}\"\n",
    "                })\n",
    "\n",
    "                pbar.update(1)\n",
    "            pbar.set_postfix({\"Status\": \"Finished\"})\n",
    "    return documents\n",
    "\n",
    "documents_raw = run_process_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Chunking and Metadata\n",
    "\n",
    "Now we'll split the extracted documents into smaller chunks suitable for embedding and indexing, and add metadata such as:\n",
    "- Chunk ID\n",
    "- Last update timestamp\n",
    "- Content tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Timestamps\n",
    "This function provides consistent timestamps for our document metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def get_sweden_time():\n",
    "    # Define the timezone for Sweden\n",
    "    sweden_tz = pytz.timezone('Europe/Stockholm')\n",
    "    utc_now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    sweden_time = utc_now.astimezone(sweden_tz)\n",
    "    return sweden_time.isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Document Splits\n",
    "This function divides documents into logical sections based on titles and headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(raw_doc):\n",
    "    document_chunks = []\n",
    "    current_title = None\n",
    "\n",
    "    data = raw_doc[\"data\"]\n",
    "\n",
    "    # Loop through paragraphs to structure the main content\n",
    "    for paragraph in data.paragraphs:\n",
    "        # Extract page number from boundingRegions\n",
    "        if paragraph.role == \"title\":\n",
    "            # Update the current title but do not add an entry to document_chunks\n",
    "            current_title = paragraph.content\n",
    "        elif paragraph.role == \"sectionHeading\":\n",
    "            # Start a new entry for the section heading with the current title\n",
    "            document_chunks.append({\n",
    "                \"title\": current_title,\n",
    "                \"content\": \"\",\n",
    "            })\n",
    "        else:\n",
    "            # Add content to the last entry, updating page_end as needed\n",
    "            if document_chunks:\n",
    "                document_chunks[-1][\"content\"] += \" \" + paragraph.content\n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_intelligence_client = initialize_document_intelligence_client()\n",
    "# operation_id = result.details[\"operation_id\"]\n",
    "\n",
    "# for doc in documents_raw:\n",
    "#     if doc['data'].figures:\n",
    "#         for figure in doc['data'].figures:\n",
    "#             if figure.id:\n",
    "#                 response = document_intelligence_client.get_analyze_result_figure(\n",
    "#                     model_id=doc['data'].model_id, result_id=doc['data'].operation_id, figure_id=figure.id\n",
    "#                 )\n",
    "#                 with open(f\"data/figures/{doc['filename']}/{figure.id}.png\", \"wb\") as writer:\n",
    "#                     writer.writelines(response)\n",
    "# # .documents_raw[0]['data'].figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Content into Smaller Chunks\n",
    "This function breaks down document content into smaller chunks suitable for embedding, using the LangChain text splitter to ensure chunks are properly sized for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks(splits, chunk_size=1024, chunk_overlap=128):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        model_name=azure_openai_chat_model\n",
    "    )\n",
    "    chunk_list = []\n",
    "\n",
    "    for doc in splits:\n",
    "        content = doc.get(\"content\", \"\")\n",
    "        \n",
    "        # Create a clean copy of metadata without content\n",
    "        doc_metadata = doc.copy()\n",
    "        if \"content\" in doc_metadata:\n",
    "            doc_metadata.pop(\"content\")\n",
    "            \n",
    "        # Create chunks from the content\n",
    "        chunks = text_splitter.create_documents([content])\n",
    "        \n",
    "        if not chunks:\n",
    "            new_doc = doc.copy()\n",
    "            chunk_list.append(new_doc)\n",
    "        else:\n",
    "            # Create a new document for each chunk\n",
    "            for chunk in chunks:\n",
    "                new_doc = doc_metadata.copy()\n",
    "                new_doc[\"content\"] = chunk.page_content\n",
    "                chunk_list.append(new_doc)\n",
    "                \n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Documents with Embeddings\n",
    "This function orchestrates the entire document processing workflow: splitting documents, creating chunks, generating embeddings, and preparing the final data format for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Merged function that combines format_documents and convert_document\n",
    "def process_documents(documents_raw, get_embedding_fn):\n",
    "    processed_documents = []\n",
    "    namespace = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")\n",
    "    \n",
    "    # Process each document with progress tracking\n",
    "    for doc in tqdm(documents_raw, desc=\"Processing documents\"):\n",
    "        url = doc[\"url\"]  # URL for the document\n",
    "        filename = doc[\"filename\"]  # Filename of the document\n",
    "\n",
    "        # Split the raw document based on title and section headings\n",
    "        splits = create_splits(doc)\n",
    "        # Create chunks from the splits if needed\n",
    "        chunks = create_chunks(splits, chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "        # Process each chunk with embeddings in a single pass\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_name = f\"{filename}_chunk_{i}\"\n",
    "            \n",
    "            # Generate a unique ID for this chunk\n",
    "            chunk_id = str(uuid.uuid5(namespace, chunk_name))\n",
    "            \n",
    "            # Get the content from the chunk\n",
    "            content = chunk.get(\"content\", \"\") if isinstance(chunk, dict) else chunk.page_content\n",
    "            \n",
    "            # Generate embedding vector for this content\n",
    "            text_vector = get_embedding_fn(content) if content else []\n",
    "            \n",
    "            # Build the complete document with all required fields\n",
    "            processed_documents.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"title\": chunk_name,\n",
    "                \"content\": content,\n",
    "                \"last_update\": get_sweden_time(),\n",
    "                \"url\": url,\n",
    "                \"text_vector\": text_vector\n",
    "            })\n",
    "            \n",
    "    return processed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Processing and Indexing Documents\n",
    "\n",
    "Now we'll execute our document processing pipeline to generate the final embedded documents ready for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Process documents and generate embeddings in one step\n",
    "data_final = process_documents(documents_raw, get_embedding)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Processing completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uploading to Azure AI Search Index\n",
    "\n",
    "### Defining Upload Function\n",
    "This function handles the upload of processed documents to our Azure AI Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "def push_to_index(data, search_credential, search_endpoint, index_name=index_name):\n",
    "    search_client = SearchClient(\n",
    "        index_name=index_name,\n",
    "        endpoint=search_endpoint,\n",
    "        credential=search_credential\n",
    "    )\n",
    "    search_client.upload_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched Upload to Azure AI Search\n",
    "Finally, we upload our processed documents to Azure AI Search in batches to ensure reliable indexing of large document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Group the header based chunks into batches\n",
    "batch_size = 5\n",
    "total_chunks = len(data_final)\n",
    "num_batches = (total_chunks + batch_size - 1) // batch_size\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Process each batch\n",
    "for i, batch_num in enumerate(tqdm(range(num_batches), desc=\"Processing Batches\")):\n",
    "    batch_start_time = time.time()\n",
    "    start = batch_num * batch_size\n",
    "    batch = data_final[start:start + batch_size]\n",
    "\n",
    "    # Push the documents to the index\n",
    "    push_to_index(\n",
    "        data=batch,\n",
    "        search_credential=search_credential,\n",
    "        search_endpoint=search_endpoint,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "overall_end_time = time.time()\n",
    "elapsed_overall_time = overall_end_time - overall_start_time\n",
    "print(f\"All batches pushed in {elapsed_overall_time:.2f} seconds to {index_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
